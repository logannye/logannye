{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMX/h6Ot/SUOX0/ABBnhYWX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/logannye/logannye/blob/main/clinical_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "### ------------------------------------------------------------------------- ###\n",
        "### -------------------------- HYPERPARAMETERS ------------------------------ ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "batch_size = 128 # how many independent sequences we will process in parallel\n",
        "block_size = 512 # the maximum context length for predictions\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6        # n_embd / n_head = # of dimensions per head\n",
        "dropout = 0.4\n",
        "n_layer = 8\n",
        "\n",
        "torch.manual_seed(1738) # Fetty, baby\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P08wVvMATCTF",
        "outputId": "4a50d82c-37ee-4535-d26c-1246d0a14b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alVk-dSKSGEW"
      },
      "outputs": [],
      "source": [
        "### ------------------------------------------------------------------------- ###\n",
        "### --------------------------- DATASET SPECS ------------------------------- ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "# you can use wget to download the dataset from the web here\n",
        "# read in your text corpus from the .txt file in your directory\n",
        "with open('/content/clinical_notes.txt', 'r', encoding='utf-8') as f: # https://github.com/socd06/medical-nlp this dataset is a corpus of ~5000 clinical notes\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characteers that occur in this corpus of text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: takes  string, outputs a list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: taks a list of ints and outputs a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.85*len(data)) # this can change depending on your taste. Here we're doing 85:15 splits\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "  # make a small batch of data inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]) # stacks up the 1-dimensional tensors of samples into rows of x\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # stacks up the targets in y\n",
        "  x = x.to(device)  # Move x to your device (specified in header - GPU vs CPU)\n",
        "  y = y.to(device)  # Move y to your device as well\n",
        "\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ------------------------------------------------------------------------- ###\n",
        "### ------------------------ SELF ATTENTION HEAD ---------------------------- ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B,T,16)\n",
        "    q = self.query(x) # (B,T,16)\n",
        "    # compute attention scores (\"affinities\")\n",
        "    wei = q @ k.transpose(-2,-1) * (C ** -0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B,T,T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggreegation of values\n",
        "    v = self.value(x) # (B,T,C)\n",
        "    out = wei @ v # (B,T,C) @ (B,T,C) -> (B,T,C)\n",
        "    return out\n",
        "\n",
        "\n",
        "### ------------------------------------------------------------------------- ###\n",
        "### ------------------------ MULTI-HEAD ATTENTION --------------------------- ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenate the outputs\n",
        "    out = self.proj(out) # linearly transform the concatenated outputs in last layer\n",
        "    return out\n",
        "\n",
        "\n",
        "### ------------------------------------------------------------------------- ###\n",
        "### ------------------------ FEED FORWARD NETWORK --------------------------- ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer following by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(n_embd, 4*n_embd),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4*n_embd, n_embd), # projection layer going back into residual pathway\n",
        "      nn.Dropout(dropout), # dropout for regularization\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "### ------------------------------------------------------------------------- ###\n",
        "### ------------------------- TRANSFORMER BLOCKS ---------------------------- ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd: embedding dimension, n_head: number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head # should be 8 in our case\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) # communication\n",
        "    self.ffwd = FeedForward(n_embd) # computation\n",
        "    self.layer_norm1 = nn.LayerNorm(n_embd)\n",
        "    self.layer_norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.layer_norm1(x)) # fork off, communicate, and then add back\n",
        "    x = x + self.ffwd(self.layer_norm2(x)) # fork off, compute, and then add back\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ui2gs37gTKje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size': batch_size, # Increase this if your GPU can handle it\n",
        "    'lr': learning_rate,\n",
        "    'epochs': max_iters,\n",
        "}"
      ],
      "metadata": {
        "id": "ZZR-yS56jUU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "import wandb\n",
        "wandb.login(key=\"8198cb5f5316ad7597e44dcf4e6b5d063b12f7e8\") # API Key is in your wandb account, under settings (wandb.ai/settings)\n",
        "\n",
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"Testing-my-Transformer\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Sz2QAR0dZAbH",
        "outputId": "8193e707-321e-4557-e8d5-7b05ec50f631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlnye\u001b[0m (\u001b[33m11-785project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240225_135711-j2nsdq64</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/11-785project/hw2p2-ablations/runs/j2nsdq64' target=\"_blank\">Testing-my-Transformer</a></strong> to <a href='https://wandb.ai/11-785project/hw2p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/11-785project/hw2p2-ablations' target=\"_blank\">https://wandb.ai/11-785project/hw2p2-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/11-785project/hw2p2-ablations/runs/j2nsdq64' target=\"_blank\">https://wandb.ai/11-785project/hw2p2-ablations/runs/j2nsdq64</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ------------------------------------------------------------------------- ###\n",
        "### --------------------------- LANGUAGE MODEL ------------------------------ ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPT()\n",
        "m = model.to(device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Log losses to WandB\n",
        "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"step\": iter})\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqms1iBPTR4I",
        "outputId": "34800cc1-3230-44e6-b83f-6b80c0ca2a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.467693 M parameters\n",
            "step 0: train loss 4.7714, val loss 4.7747\n",
            "step 500: train loss 2.1342, val loss 2.1948\n",
            "step 1000: train loss 1.4814, val loss 1.5642\n",
            "step 1500: train loss 1.1967, val loss 1.2757\n",
            "step 2000: train loss 1.0520, val loss 1.1331\n",
            "step 2500: train loss 0.9797, val loss 1.0579\n",
            "step 3000: train loss 0.9261, val loss 1.0083\n",
            "step 3500: train loss 0.8868, val loss 0.9696\n",
            "step 4000: train loss 0.8573, val loss 0.9431\n",
            "step 4500: train loss 0.8333, val loss 0.9174\n",
            "step 5000: train loss 0.8123, val loss 0.8977\n",
            "step 5500: train loss 0.7924, val loss 0.8840\n",
            "step 6000: train loss 0.7763, val loss 0.8645\n",
            "step 6500: train loss 0.7618, val loss 0.8500\n",
            "step 7000: train loss 0.7509, val loss 0.8442\n",
            "step 7500: train loss 0.7380, val loss 0.8292\n",
            "step 8000: train loss 0.7235, val loss 0.8196\n",
            "step 8500: train loss 0.7138, val loss 0.8106\n",
            "step 9000: train loss 0.7043, val loss 0.8015\n",
            "step 9500: train loss 0.6954, val loss 0.7929\n",
            "\n",
            "HISTORY: Herpefal medical attempts and reviewed alcohold by Dr. Basically by XYZ name with her count of day.  Because of his intravascular herlives, goiter and ankletal drugs.,SOCIAL HISTORY:, Herself smokes airway is 13 yesterday, alcohols over.  Her maternal bad breast motired age 7, and perceiae else perinepherine diarrhea 71.  His feet children and Hasson wesher she has very other pareololw.  AXIS IV:  Negative.,EYES AND NECK:  After nontender, postural erythema, postturessing or superile discomfort without pain to be turns.  The instrument was putting good hemodynamia.  The anterior posterior portion of the abdomen was present, so undergoes surgery ANALY DEV disc.  At this time, so, the liver appearently not open liver, needle prep stabilization.  Disc, and dorsal expansion was mobilized unilate to pathology it.  Entering tongue and facet and focal there were intact, attention was then placed in the epightht under 4 to monitor.  In patient, the patient's adherence was supposed com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix your weights and biases run. And change the model name to 'GPT' instead of bigramLanguageModel"
      ],
      "metadata": {
        "id": "wqo0t3-l2_3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=3000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__CxK8yCqVV",
        "outputId": "f7a84ba8-a552-492d-919e-cfbb12b4a700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " PNeurosurble done 2/18/96, thrombed 9/6 to 28/7/92 with colonoscopy felt wires that is three year and than 8/30 minutes has previously dularied atrium and tenderness.,SENSORY:  ,Fever Analged and nursing facets have fever any limited mitral odoing listed as necessary.  There is no repropressive articular symptoms in the emergency deficits at home.  No thyros only the central condyle, attending into the emergency room and in the emergency depidurity of the emergency department with school XVA, with burition CT scan on 08/10/29, rapuls with right-sided lumbar puncta-by pulmonary sepsis.  There is no concerns of pancreas in the emergency desiccation to the intentional.  Examination reveals a rectal latent latencille, but it showed attended forms and it show any fasciculation in the apex of the carotid ointmenant.  Memoning is covered before being of Studie.,PLAN: , The risks, benefits, alternatives of the transfusion, common unchange by CSF.,PROCEDURE IN DETAIL: , After appropriate size antibiotics and brought to the procedure well and with a history of paradiaspectory protanchy.  The risks and benefits of expected bleeding, complications.  Heavy, heemoperation and the patient was discharged home.  Postoperative for bleeding, infection, or the protectus (as well as follow up with an osteophyte at times body anging.  She was also started with some vital signs of informedial postoperative, the patient is marked over him.  Here opiated by Brood pressure.  He is unit on from that a history of sleep apnea and also today is on a damage to his one-to-course comes on the patient that he is brought up on GI ure.  However, he had been was hyperactivitued.  Skin once daily, the followed by handed her only as he mean to make summe denstition signal way.  He structually interested a markedly small prior to the siglate really decrease extending.  last severe.  Once for thoracic pad in her mother reports, it walke with his way does oppen a day.  He developed a GI was 5.6 immobilize his motor repaired and distended in the calf progress.  The prior his primary bladder in her uninter head transported to the ICU and these requested a 12.5 area in their mobilization of the maximum and stenosis.  We irrigated by 2 to pass the surlaked the uterus was not further distal tight.  I was left knee then dissected to as a rash amount of compresse.  The catheter was 2.7 mm/sure caliber were placed into the lunar ned.  The path and superimedial pressure were noted to be nurespace.  A hemostat below the hemoglobin and it was also ensure caliber with a left mainle in the closure although securely the base of retraction at the case above and a soft tissue was done, which began with solid interous a tear-occlusive procedure.,PROCEDURE DETAIL: , After approximately his began his pedis, we widiowed about all questionable to aleven one-lateral procedure.  The nasopharynx was found to be correct.  We had yexpress the nasopharynx and transferred to UIHC folds 3 years and skin in the mid \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ------------------------------------------------------------------------- ###\n",
        "### -------------------------- TEXT OUTPUT FILE ----------------------------- ###\n",
        "### ------------------------------------------------------------------------- ###\n",
        "\n",
        "# Uncomment to save the output to a new file\n",
        "!mkdir '/content/data'\n",
        "open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIJtwyXXSVpk",
        "outputId": "3b1ab14f-9cb7-4781-81ad-98b71edb1781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/data’: File exists\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5001"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), '/content/clinicalGPT_decoder_only')"
      ],
      "metadata": {
        "id": "T7oLy6UQCj17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file to your local machine\n",
        "files.download('/content/clinicalGPT_decoder_only')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rNmR7igKSwma",
        "outputId": "f313be21-c087-4085-d010-2427fcbf888b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_25e9cd6d-a9a8-4128-a340-9dcd403e3b92\", \"clinicalGPT_decoder_only\", 108312384)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LATER ON, IF YOU WANT TO RELOAD THE MODEL AND WEIGHTS\n",
        "\n",
        "i.e., for inference or to continue training"
      ],
      "metadata": {
        "id": "imaVqZsKSzpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = BigramLanguageModel()  # Make sure this is the same model architecture\n",
        "\n",
        "# Load the saved state_dict\n",
        "model.load_state_dict(torch.load('path_to_save/your_model_name.pth'))\n",
        "\n",
        "# If you're doing inference only, switch to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "gUP3X1ylS78A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}